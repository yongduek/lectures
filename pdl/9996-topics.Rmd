# Incomplete Study Topics

## Recurrent Models of Visual Attention

- [Visual Attention Model in Deep Learnin, towardsdatascience.com](https://towardsdatascience.com/visual-attention-model-in-deep-learning-708813c2912c)

- [Recurrent Models of Visual Attention by Mnih, etal, 2014](https://arxiv.org/abs/1406.6247)

```{r RAMModel, echo=FALSE, fig.align='center', fig.cap='The Recurrent Attention Model (RAM)'}
knitr::include_graphics('./figures/RAM-Model.png')
```

## DVAP: Deep Visual Attention Prediction
- [Deep Visual Attention Prediction, Wenguan Wang, TIP2018](https://arxiv.org/pdf/1705.02544.pdf)
```{r DVAP, echo=FALSE, fig.align='center', fig.cap='Deep Visual Attention Prediction'}
knitr::include_graphics('./figures/DVAP-TIP2018.png')
```


## Attention U-Net, 2018 Medical
- Additive attention gate (AG) is proposed.
- [Attention U-Net Learning Where to look for the Pancreas, 2018](https://openreview.net/pdf?id=Skft7cijM)
- [pytorch Source code is here](https://github.com/ozan-oktay/Attention-Gated-Networks)

```{r agnet, echo=FALSE, fig.align='center', fig.cap='Attention U-Net'}
knitr::include_graphics('./figures/attention-unet-3.png')
```
```{r , echo=FALSE, fig.align='center', fig.cap='Attention U-Net'}
knitr::include_graphics('./figures/attention-unet-1.png')
```
```{r , echo=FALSE, fig.align='center', fig.cap='Attention Gate in Attention U-Net'}
knitr::include_graphics('./figures/attention-unet-2.png')
```





## Attend and Rectify, ECCV 2018

As a result, Wide Residual Networks
augmented with our proposal surpasses the state of the art classification
accuracies in CIFAR-10, the Adience gender recognition task, Stanford
dogs, and UEC Food-100.

- [Attend and Rectify: A gated attention mechanism for fine-grained recovery, ECCV 2018](http://openaccess.thecvf.com/content_ECCV_2018/papers/Pau_Rodriguez_Lopez_Attend_and_Rectify_ECCV_2018_paper.pdf)
- [pytorch code is here](https://github.com/prlz77/attend-and-rectify)
```{r aar, echo=FALSE, fig.align='center', fig.cap='Attend and Rectify'}
knitr::include_graphics('./figures/aar-eccv2018.png')
```



## Guided Attention Inference Network, CVPR 2018

- [Tell me where to look: Guided Attention Inference Network](http://openaccess.thecvf.com/content_cvpr_2018/papers/Li_Tell_Me_Where_CVPR_2018_paper.pdf)

- [Chainer implementation by someone, Guided Attention for FCN](https://github.com/alokwhitewolf/Guided-Attention-Inference-Network)

```{r GAIN, echo=FALSE, fig.align='center', fig.cap='Guided Attention Inference Network'}
knitr::include_graphics('./figures/gain-cvpr2018.png')
```
```{r GAINext, echo=FALSE, fig.align='center', fig.cap='GAIN-ext, to include pixel-level annotations.'}
knitr::include_graphics('./figures/gainext-cvpr2018.png')
```



## PSA Net

 Our proposed approach achieves
top performance on various competitive scene parsing datasets, including ADE20K, PASCAL VOC 2012 and Cityscapes, demonstrating its
effectiveness and generality.

- [PSANet: Point-wise Spatial attention network for scene parsing, H. Zhao, ECCV 2018](https://hszhao.github.io/papers/eccv18_psanet.pdf)

- [github, caffee](https://github.com/hszhao/PSANet)

```{r PSANet, echo=FALSE, fig.align='center', fig.cap='PSA Net'}
knitr::include_graphics('./figures/psanet-eccv2018.png')
```
