---
output:
  pdf_document: default
  html_document: default
---

# Object Detection

- Check out [Dive into Deep Learning](http://d2l.ai/chapter_computer-vision/index.html)

- Q. Try to figure out the size of receptive field for VGG-16 network.


## mAP: mean Average Precision

- refer to  [Object Detection Metrics by rafaelpadilla@github.com](https://towardsdatascience.com/breaking-down-mean-average-precision-map-ae462f623a52)

* Precision : ratio of true positives to the number of predictions marked as True
\[
    P = \frac{TP}{TP + FP}
\]
* Recall = TPR = True Positve Rate = Sensitivity
\[
    R = \frac{TP}{TP+FN}
\]

* AP : Average Precision = AUC = Area Under Precison-Recall Curve
```{r, echo=FALSE, fig.align='center', fig.cap='Precision-Recall Curve. The area is approximated as shown, which gives the AP for this curve. An IoU threshold values should be fixed and given so that this graph may be constructed.'}
knitr::include_graphics('figures/interpolated_precision-AUC_v2.png')
```

* mAP: mean Average Precision: AP is averaged over all class categories, give.

* `mAP@[.5,.95]` means average mAP over different IoU thresholds for [0.5:0.05:0.95] - from [stackexchange.com](https://datascience.stackexchange.com/questions/16797/what-does-the-notation-map-5-95-mean)
    - There is an associated MS COCO challenge with a new evaluation metric, that averages mAP over different IoU thresholds, from 0.5 to 0.95, step 0.05. See [Inside-Outside Net: Detecting Objects in Context with Skip Pooling and RNN by Sean Bell, Zitnick, Bala, Girshick, 2016, CVPR](https://www.cs.cornell.edu/~sbell/pdf/cvpr2016-ion-bell.pdf)
    - We evaluate the mAP averaged for IoU $\in [0.5:0.05:0.95]$ (COCO's standard metric, simply denoted as mAP@[.5, .95]) - [Faster RCNN ](https://arxiv.org/pdf/1506.01497.pdf)
    - To evaluate our final detections, we use the official COCO API, which measures mAP averaged over IoU thresholds in [0.5:0.05:0.95], amongst other metrics. [Speed/accuracy trade-offs for modern convolutional object detectors, Huang, et.al, 2017](https://arxiv.org/pdf/1611.10012.pdf) Figure shows a performance evaluation of various architectures.
```{r, echo=FALSE, fig.align='center', fig.cap='Accuracy of detector (mAP on COCO) vs accuracy of feature extractor (as measured by top-1 accuracy on ImageNet-CLS)'}
knitr::include_graphics('figures/obj-det-accuracy.png')
```
    BTW, the [source code](https://github.com/cocodataset/cocoapi/blob/master/PythonAPI/pycocotools/cocoeval.py#L501) of coco shows exactly what mAP@[.5:.95] is doing:
`self.iouThrs = np.linspace(.5, 0.95, np.round((0.95 - .5) / .05) + 1, endpoint=True)`


    
    
## Bounding Boxes, Anchor Boxes

- Anchor boxes mean a set of boxes defined for an input image location.
- An anchor box is a hypothetic box that an object may exist inside. The box's anchor location is the image location.
- Since the size of the objects may vary from object to object or from an image scale to scale, we consider multiple anchor boxes at an image location (e.g., 5 for the figure below).

```{python bbox, code=readLines('python/bbox.py')}
```
* So the image location (250,250) has 5 anchor boxes. Other image locations will also have 5 boxes each.

## IoU: Intersection over Union
```{r iouFigure, fig.align='center', fig.cap='IoU = Intersection over Union'}
knitr::include_graphics('figures/iou.png')
```


## Labeling Training Set Anchor Boxes

In order to train the object detection model, we need to mark two types of GT data for each anchor box:
1. (1 category) the category of the target contained in the anchor box
2. (4 offsets) the offset of the gt bbox relative to the anchor box.

In prediction stage, the network produces
1. (category, 4 offsets) for each anchor box,
2. then the anchor box position is adjusted according to the predicted offsets,
3. filter out the prediction bounding boxes (choose only appropriate ones).

- In the training set for object detection, each image is labelled with the location of the gt bounding box and the category of the target contained.
- So how do we assign gt bboxes to anchor boxes similar to them?

```{r anchorLabel, fig.align='center', fig.cap='Assigning GT bounding boxes to anchor boxes. From left to right, it shows 3 steps.'}
knitr::include_graphics('figures/anchor-label.png')
```

1. Build a matrix $X$ of size $n_a \times n_b$ where $n_a$ is the num. of anchor boxes and $n_b$ is the num. of gt bboxes. $x_{ij}$ is the IoU.
2. Find the largest element in $X$. It is the assignment pair obtained.
    - The IoU value must be greater than a predetermined threshold. Otherwise, no further assigning is necessary.
    - Then this gt box cannot be assigned to any other anchors. Disable all the rows and columns corresponding to this.
    - The left matrix in the figure shows the current status.
3. repeat 2 until all the categories are assigned.
4. For the remainings: for each anchor, assign a gt box of the largest IoU if it is greater than the threshold. Otherwise, assign 'background' category.

* For all the (A, G) pairs obtained from this assignment operation, we can record the category of G and compute the offset of A from G.
* All other anchors are marked as background (no-object). Offsets are not necessary.
* The offsets are recorded using the following equation:
\[
    \bigg(
        s_x \frac{x_g - x_a}{w_a},\ 
        s_y \frac{y_g - y_a}{h_a},\ 
        s_w \log (w_g / w_a) , \ 
        s_h \log (h_g / h_a)
    \bigg)
\]
where $s_x = s_y = 10$ and $s_w = s_h = 5$, which are user-defined scale constants.

* Notice that these 4 offset values are the quantities that our NN should predict for the outputs of the corresponding anchors.


## NMS: Output Bounding Boxes for Prediction
