---
output:
  pdf_document: default
  html_document: default
---

# Generative Adversarial Network

- GAN Applications are well summarized in [here](https://github.com/nashory/gans-awesome-applications)

- A tutorial on GAN, pix2pix, CycleGAN is in [microsoft blob](https://www.microsoft.com/developerblog/2017/06/12/learning-image-image-translation-cyclegans/)

- A fancy article about pix2pix is [here](https://affinelayer.com/pix2pix/)

- A youtube talk by one of the authors of cycleGAN in Korean [here](https://www.youtube.com/watch?v=Fkqf3dS9Cqw&t=2401s)
    - and its [summary](http://www.kwangsiklee.com/2018/03/cyclegan%EC%9D%B4-%EB%AC%B4%EC%97%87%EC%9D%B8%EC%A7%80-%EC%95%8C%EC%95%84%EB%B3%B4%EC%9E%90/)

It is always good to read the original paper
- [Goodfellow](https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf)
- [DCGAN ](https://arxiv.org/pdf/1511.06434.pdf)

```{r, echo=FALSE, out.width='80%', fig.align='center', fig.cap='Illustration of GAN with ConvNets.'}
knitr::include_graphics('figures/gan.png')
```

The meaning of the loss function $V(D,G)$:

- $\log(D(x))$ : this term must be maximized so that $D$'s discrimination power is big and produces probability $1$ whenever the input is from real.
- $\log(D(G(z)))$: Th generator $G$ tries to produce realistic outputs so that $D$ cannot discriminate and therefore produce a probability close to $1$. Therefore, the loss $\log(D(G(z)))$ should be maximized **on behalf of $G$**. Conversely, $\log(1-D(G(z)))$ must be minimized to fulfill the same purpose.
- Therefore, the problem is called a minimax game in which $D$ tries to maximize and $G$ tries to minimize the loss function $V(D,G)$:

\begin{equation}
    V(D,G) = \mathbb{E}_{x\sim p_\mathrm{data}(x)} \log[ D(x) ] \quad + \quad \mathbb{E}_{z\sim p_z(z)} \log [1-D(G(z))]
\end{equation}





## GAN with Fully Connected Network

The source code is from ['github/yunjey/pytorch-tutorial'](https://raw.githubusercontent.com/yunjey/pytorch-tutorial/master/tutorials/03-advanced/generative_adversarial_network/main.py).


This code is very concise and helpful for us to understand the mechanism of GAN. 
Let's see one by one.

First we declare headers.
```
import os
import torch
import torchvision
import torch.nn as nn
from torchvision import transforms
from torchvision.utils import save_image


# Device configuration
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
```

Now some hyper parameters. 
```
# Hyper-parameters
latent_size = 64
hidden_size = 256
image_size = 784
batch_size = 100
sample_dir = './data/gan-mnist'

# Create a directory if not exists
if not os.path.exists(sample_dir):
    os.makedirs(sample_dir)
```

* `latent_size` is the dimension of the input random variable, which is 64. The value 64 is the author's choice and you may choose a different one, smaller or larger. It's implication is that the set of the input images (mnist image data set) is assumed to be of Gaussian distribution in 64 dimensional space.

* `image_size = 784` is the number of total pixels in an image of the data set. It is obtained from `28x28`. The GAN network designed in the code does not use 2d convolutional network. Instead, it uses a fully 1d connected network. Therefore, the input image of size $28\times28$ has to be converted to a one dimension vector of size 784. 


Now data loader is coming.
```
# Image processing
transform = transforms.Compose([
                transforms.ToTensor(),
                transforms.Normalize(mean=(0.5, 0.5, 0.5),   # 3 for RGB channels
                                     std=(0.5, 0.5, 0.5))])

# MNIST dataset
mnist = torchvision.datasets.MNIST(root='./data/',
                                   train=True,
                                   transform=transform,
                                   download=True)

# Data loader
data_loader = torch.utils.data.DataLoader(dataset=mnist,
                                          batch_size=batch_size, 
                                          shuffle=True)
```

* `ToTensor()` will transform the image shape from `28x28` to `1x28x28` which is the required type for pytorch image data (even though the network here does not use 2d convnet).
* Notice that any image is transformed so that the pixel values are in the range of $[-1,1]$
* `data_loader` will provide a batch of size 100 as defined before.


Now let's defind `D` the discriminator and `G` the generator.
```
# Discriminator
D = nn.Sequential(
    nn.Linear(image_size, hidden_size),
    nn.LeakyReLU(0.2),
    nn.Linear(hidden_size, hidden_size),
    nn.LeakyReLU(0.2),
    nn.Linear(hidden_size, 1),
    nn.Sigmoid())

# Generator 
G = nn.Sequential(
    nn.Linear(latent_size, hidden_size),
    nn.ReLU(),
    nn.Linear(hidden_size, hidden_size),
    nn.ReLU(),
    nn.Linear(hidden_size, image_size),
    nn.Tanh())

# Device setting
D = D.to(device)
G = G.to(device)
```

* Discriminator, `D`, gets a batch of vectors of size `image_size=784`. Even though the name is `image_size`, it has nothing to do with image processing in this example. 2D convolutional network will be used in the next section.
* The output of `D` is from `nn.Sigmoid()`. Therefore the output is in `[0,1]` indicating the probability of the input being real.
* Generator `G` gets a random vector of size `latent_size` and produces a vector of size `image_size` whose elements are in the range `[-1,1]` controled by `nn.Tanh()`.



Now the loss function and two optimizers for the `D` network and `G` network.
```
# Binary cross entropy loss and optimizer
criterion = nn.BCELoss()

d_optimizer = torch.optim.Adam(D.parameters(), lr=0.0002)
g_optimizer = torch.optim.Adam(G.parameters(), lr=0.0002)
```

* Note that `d_optimizer` is given the parameters of `D` only. Likewise, `g_optimizer` is given those of `G` only.


Some helper functions.
```
def denorm(x):
    out = (x + 1) / 2
    return out.clamp(0, 1)

def reset_grad():
    d_optimizer.zero_grad()
    g_optimizer.zero_grad()
```


Now it's time to look at the training loop, which looks rather long.
```
def train_loop (num_epochs):
    total_step = len(data_loader)
    for epoch in range(num_epochs):
        for i, (images, _) in enumerate(data_loader):
            images = images.reshape(batch_size, -1).to(device)

            # Create the labels which are later used as input for the BCE loss
            real_labels = torch.ones(batch_size, 1).to(device)
            fake_labels = torch.zeros(batch_size, 1).to(device)

            # ================================================================== #
            #                      Train the discriminator                       #
            # ================================================================== #

            # Compute BCE_Loss using real images where BCE_Loss(x, y): - y * log(D(x)) - (1-y) * log(1 - D(x))
            # Second term of the loss is always zero since real_labels == 1
            outputs = D(images)
            d_loss_real = criterion(outputs, real_labels)
            real_score = outputs

            # Compute BCELoss using fake images
            # First term of the loss is always zero since fake_labels == 0
            z = torch.randn(batch_size, latent_size).to(device)
            fake_images = G(z)
            outputs = D(fake_images)
            d_loss_fake = criterion(outputs, fake_labels)
            fake_score = outputs

            # Backprop and optimize
            d_loss = d_loss_real + d_loss_fake
            reset_grad()
            d_loss.backward()
            d_optimizer.step()

            # ================================================================== #
            #                        Train the generator                         #
            # ================================================================== #

            # Compute loss with fake images
            z = torch.randn(batch_size, latent_size).to(device)
            fake_images = G(z)
            outputs = D(fake_images)

            # We train G to maximize log(D(G(z)) instead of minimizing log(1-D(G(z)))
            # For the reason, see the last paragraph of section 3. https://arxiv.org/pdf/1406.2661.pdf
            g_loss = criterion(outputs, real_labels)

            # Backprop and optimize
            reset_grad()
            g_loss.backward()
            g_optimizer.step()

        # Save real images
        if (epoch+1) == 1:
            images = images.reshape(images.size(0), 1, 28, 28)
            save_image(denorm(images), os.path.join(sample_dir, 'real_images.png'))

        # Save sampled images
        fake_images = fake_images.reshape(fake_images.size(0), 1, 28, 28)
        save_image(denorm(fake_images), os.path.join(sample_dir, 'fake_images-{}.png'.format(epoch+1)))
        # batch
    return
#

num_epochs = 200
train_loop (num_epochs)

# Save the model checkpoints 
torch.save(G.state_dict(), 'G.ckpt')
torch.save(D.state_dict(), 'D.ckpt')

print ('Finished.')
```

* Note that in the following code block,
    1. the code `(images, _)` means that we don't need the labels of the images.
    1. the images are reshaped to the shape of `(batch_size, 784)`
    
```
def train_loop (num_epochs):
    total_step = len(data_loader)
    for epoch in range(num_epochs):
        for i, (images, _) in enumerate(data_loader):
            images = images.reshape(batch_size, -1).to(device)
```


The meaning of the loss function $V(D,G)$:

- $\log(D(x))$ : this term must be maximized so that $D$'s discrimination power is big and produces probability $1$ whenever the input is from real.
- $\log(D(G(z)))$: Th generator $G$ tries to produce realistic outputs so that $D$ cannot discriminate and therefore produce a probability close to $1$. Therefore, the loss $\log(D(G(z)))$ should be maximized **on behalf of $G$**. Conversely, $\log(1-D(G(z)))$ must be minimized to fulfill the same purpose.
- Therefore, the problem is called a minimax game in which $D$ tries to maximize and $G$ tries to minimize the loss function $V(D,G)$:

$$
    V(D,G) = \mathbb{E}_{x\sim p_\mathrm{data}(x)} \log[ D(x) ] \quad 
        + \quad \mathbb{E}_{z\sim p_z(z)} \log [1-D(G(z))]
$$

**Training the discriminator** is done by the following part of the code.

The `BCE_Loss()` function computes the binary cross-entropy loss defined as follows.

$$
    \mathrm{BCELoss}(x,y) = - y * \log(D(x)) - (1-y) * \log(1 - D(x))
$$

Note that

    1. the second term of the loss is always zero for `real_labels == 1`.
    1. the first term of the loss is always zero for `fake_labels == 0`.

In other words, the following code will maximize

\begin{equation}
    \log(D(x)) + \log (  1 - D(G(z)) )
\end{equation}

with respect to the parameters of `D`. The first term is due to images of `real_labels` and the second due to the images of `fake_labels`.
```
            # BCE_Loss using the real images
            outputs = D(images)
            d_loss_real = criterion(outputs, real_labels)
            real_score = outputs

            # Compute BCELoss using fake images
            z = torch.randn(batch_size, latent_size).to(device)
            fake_images = G(z)
            outputs = D(fake_images)
            d_loss_fake = criterion(outputs, fake_labels)
            fake_score = outputs

            # Backprop and optimize
            d_loss = d_loss_real + d_loss_fake
            reset_grad()
            d_loss.backward()
            d_optimizer.step()
```

In the code, the loss for `D` is given by the sum of the two losses, one by real and the other by fake images.
```
            d_loss = d_loss_real + d_loss_fake
```
* Then its gradient is computed by `d_loss.backward()` w.r.t the parameters of `D` due to `d_loss_real` and the parameters of `G` due to `d_loss_fake`.
* However, `d_optimizer` considers only the parameters of `D` because it is defined as such.


Now we talk about **the generator**. The goal of optimization is to deceive `D` by generating a good output. For this we must maximize the loss $\log(D(G(z)))$ in terms of the parameters of `G`, or equivalently, minimze the loss $\log( 1 - D(G(z)) )$.
```
            # Compute loss with fake images
            z = torch.randn(batch_size, latent_size).to(device)
            fake_images = G(z)
            outputs = D(fake_images)

            # We train G to maximize log(D(G(z)) instead of minimizing log(1-D(G(z)))
            # For the reason, see the last paragraph of section 3. https://arxiv.org/pdf/1406.2661.pdf
            g_loss = criterion(outputs, real_labels)

            # Backprop and optimize
            reset_grad()
            g_loss.backward()
            g_optimizer.step()
```
Notice that we generate `fake_images` but use `real_labels` in `criterion`. Why? 

- The reason is that our goal is to upgrade `G` in the direction that `D` produces `1` or `real` as `D`'s output. Therefore the creterion uses `(fake_images, real_labels)` and maximize `\log( D(G(z)) )`. 

- In other words, when the generator tries to deceive the discriminator `D`, it will provide its images with fake labels '1'. Then the criterion produces some loss, its discrepency will be back propagated by gradients (`backward()`) and `g_optimizer` learns the responce of `D` through a gradient descent `step()`.


END.