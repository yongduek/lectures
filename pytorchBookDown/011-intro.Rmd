---
output:
  pdf_document: default
  html_document: default
---
# Introduction {#intro}




The codes here are from [https://pytorch.org/tutorials/beginner/pytorch_with_examples.html](Learning Pytorch with Examples)

The python codes are modified just a little bit to fit in the book format; instead of printing out values, their 2d plots are drawn.

Make sure that all the required modules & packages are installed in your machine.


## Warm-up numpy

Let's make a small network model with `numpy`.

```{python}
import sys
import numpy as np
import matplotlib.pyplot as plt

# N is batch size; D_in is input dimension;
# H is hidden dimension; D_out is output dimension.
N, D_in, H, D_out = 64, 1000, 100, 10

# Create random input and output data
x = np.random.randn(N, D_in)
y = np.random.randn(N, D_out)

# Randomly initialize weights
w1 = np.random.randn(D_in, H)
w2 = np.random.randn(H, D_out)

learning_rate = 1e-6
niter = 20
res = []
for t in range(niter):
    # Forward pass: compute predicted y
    h = x.dot(w1)
    h_relu = np.maximum(h, 0)
    y_pred = h_relu.dot(w2)

    # Compute and print loss
    loss = np.square(y_pred - y).sum()
    #print(t, loss)
    res.append (loss)
    
    # Backprop to compute gradients of w1 and w2 with respect to loss
    grad_y_pred = 2.0 * (y_pred - y)
    grad_w2 = h_relu.T.dot(grad_y_pred)
    grad_h_relu = grad_y_pred.dot(w2.T)
    grad_h = grad_h_relu.copy()
    grad_h[h < 0] = 0
    grad_w1 = x.T.dot(grad_h)

    # Update weights
    w1 -= learning_rate * grad_w1
    w2 -= learning_rate * grad_w2
#

plt.plot (range(niter), res, '-o')
plt.ylabel ('loss')
plt.xlabel ('epoch')
plt.show()

```

So, what is happening here?

- The network will have 64 input items: `N=64`. This is called a *batch* or *mini_batch* because it is small.

- The network's input is of dimension 1000: `D_in=1000`
- The network has one hidden layer of size 100: `H=100`
- The network's output is of dimension 10: `D_out = 10`
```{python}
# N is batch size; D_in is input dimension;
# H is hidden dimension; D_out is output dimension.
N, D_in, H, D_out = 64, 1000, 100, 10
```

Then, the input matrix $x$ of size `N x D_in` is created randomly since here we do not want to be bothered how to get it from some source. The output matrix $y$ corresponding to the input $x$ is randomly created too. We have N=64 correspondences, and we want to find a function $f$ that maps $y_i = f(x_i)$ for $i=1,...,N$.
```{python}
# Create random input and output data
x = np.random.randn(N, D_in)
y = np.random.randn(N, D_out)
```

Now we create two networks $w_1$ and $w_2$ that transfers from input to the hidden layer and from the hidden layer to the output.

```{python}
# Randomly initialize weights
w1 = np.random.randn(D_in, H)
w2 = np.random.randn(H, D_out)
```

Q: What are the functions of `numpy`  for 

1. random integer generation
1. random uniform generation
1. random Gaussian generation

The function $f$ that we want to seek is parametrized by several steps:

1. linear mapping, $w_1$, a matrix of size $1000\times100$ 
2. relu: *Rectifier Linear Unit*
3. linear mapping, $w_2$, another weight matrix of size $100\times10$

The procedure is encoded in python codes with `numpy` functions as follows

```{python eval=FALSE}
# Forward pass: compute predicted y
h = x.dot(w1)
h_relu = np.maximum(h, 0)
y_pred = h_relu.dot(w2)
```

Q: what is the mathematical meaning of them: `x.dot(w1)`, `np.maximum(h,0)`, `h_relu.dot(w2)` ?  
Describe them using the notations of linear algebra.

The error or loss for the predicted value `y_pred` from the model is the sum of the squared differences:
\begin{equation}
    \mathrm{loss} = \sum_{i=0}^{N-1}  (y_\mathrm{pred} - y )^2
\end{equation}

```{python eval=FALSE}
# Compute and print loss
loss = np.square(y_pred - y).sum()
#print(t, loss)  # orignial source prints out the values. Here it will be displayed in the figure
res.append (loss)
```

One of the most important *hidden* part of the neural network modeling is to compute the 
gradient of the loss function $\mathrm{loss}$ with respect
to each of the variables. Here we must compute

\begin{equation}
    \frac{\partial \mathrm{loss}}{\partial w_1}  \quad \frac{\partial \mathrm{loss}}{\partial w_2}
\end{equation}

The python code below computes the gradients step by step.

\begin{equation}
    \frac{\partial \mathrm{loss}}{\partial w_1} = 
        \frac{\partial \mathrm{loss}}{\partial y_\mathrm{pred}}
        \frac{\partial y_\mathrm{pred}}{\partial w_1}
\end{equation}

which goes down to ...

Q: Manually calculate the gradients for the following codes of python.

```{python eval=FALSE}
# Backprop to compute gradients of w1 and w2 with respect to loss
grad_y_pred = 2.0 * (y_pred - y)
grad_w2 = h_relu.T.dot(grad_y_pred)
grad_h_relu = grad_y_pred.dot(w2.T)
grad_h = grad_h_relu.copy()
grad_h[h < 0] = 0
grad_w1 = x.T.dot(grad_h)
```

Final step of deep neural network is updating the variables $w_1$ and $w_2$ by 
adding fractions of the gradients weighted by the user-supplied parameter $learning_rate$.

```{python eval=FALSE}
# Update weights
w1 -= learning_rate * grad_w1
w2 -= learning_rate * grad_w2
```

The code shows the basic principle of the gradient descent method. The learning rate shold be small enough for the loss to decrease and converge to a local minimum.

Q: Change `learning_rate` to other values and observe the graphs. Try larger values, and compare the graphs.