## With Pytorch Tensors

Simply speaking, **Tensor = Numpy Arry**


To run the code, you must have `pytorch` installed.

```{python}
import torch

dtype = torch.float   # data type
device = torch.device("cpu") # which device to run the code
# device = torch.device("cuda:0") # Uncomment this to run on GPU

# N is batch size; D_in is input dimension;
# H is hidden dimension; D_out is output dimension.
N, D_in, H, D_out = 64, 1000, 100, 10

# Create random input and output data
x = torch.randn(N, D_in, device=device, dtype=dtype)
y = torch.randn(N, D_out, device=device, dtype=dtype)

# Randomly initialize weights
w1 = torch.randn(D_in, H, device=device, dtype=dtype)
w2 = torch.randn(H, D_out, device=device, dtype=dtype)

learning_rate = 1e-6
epochs = 30
res = []
for t in range(epochs):
    # Forward pass: compute predicted y
    h = x.mm(w1)
    h_relu = h.clamp(min=0)
    y_pred = h_relu.mm(w2)

    # Compute and print loss
    loss = (y_pred - y).pow(2).sum().item()
    #print(t, loss)
    res.append (loss)
    
    # Backprop to compute gradients of w1 and w2 with respect to loss
    grad_y_pred = 2.0 * (y_pred - y)
    grad_w2 = h_relu.t().mm(grad_y_pred)
    grad_h_relu = grad_y_pred.mm(w2.t())
    grad_h = grad_h_relu.clone()
    grad_h[h < 0] = 0
    grad_w1 = x.t().mm(grad_h)

    # Update weights using gradient descent
    w1 -= learning_rate * grad_w1
    w2 -= learning_rate * grad_w2
#    
print ('loss = {} after epochs {}'.format(loss, epochs))

import matplotlib.pyplot as plt
fig, ax = plt.subplots();
ax.plot (range(1,1+epochs), res, '-o')
plt.show()
```

The package we use for the implementation of deep learning algorithm is `pytorch` with version greater than 4.0. In python, we import `torch` which is the original name of it.

```
import torch
```

Normally, almost all the data type in `torch` is `torch.float` which is the 32 bit floating point data type.
```{python, eval=FALSE}
dtype = torch.float   # data type
```

If you have an Nvidia cuda device such as GTX 1080, then you can use its parallel processing power.
Otherwise, CPU is the only option.

```{python, eval=FALSE}
device = torch.device("cpu") # which device to run the code
# device = torch.device("cuda:0") # Uncomment this to run on GPU
```


We used `np.randn()` and now we use `torch.randn()` with two more options.
```
# Create random input and output data
x = torch.randn(N, D_in, device=device, dtype=dtype)
y = torch.randn(N, D_out, device=device, dtype=dtype)

# Randomly initialize weights
w1 = torch.randn(D_in, H, device=device, dtype=dtype)
w2 = torch.randn(H, D_out, device=device, dtype=dtype)
```

Let's see the dimension of $x$ and $y$:
```{python}
print (x.shape, y.shape)
```

```{python}
print (x)
```

Q: What is the role of `mm()` function below?

```{python}
# Forward pass: compute predicted y
h = x.mm(w1)
h_relu = h.clamp(min=0)
y_pred = h_relu.mm(w2)

print ('h: ', h.shape, 'y_pred: ', y_pred.shape)
```

Q: Can you guess what the following code is calculating?
```{python}
# Compute and print loss
loss = (y_pred - y).pow(2).sum().item()
print ('type(loss) = ', type(loss))
```

Q: What is the role of `item()` above?


The whole computation was done in the form of Tensor object defined in `torch`.
```{python}
tensor_loss = (y_pred - y).pow(2).sum()
print ('type(tensor_loss) = ', type(tensor_loss), tensor_loss.shape, 'tensor_loss=', tensor_loss)
```


The rest of the process of computing is the same as before: 

1. Computing the **gradient** of the loss function
2. and updating the model parameters $w_1$ and $w_2$ in the **negative** gradient direction.

```{python}
# Backprop to compute gradients of w1 and w2 with respect to loss
grad_y_pred = 2.0 * (y_pred - y)
grad_w2 = h_relu.t().mm(grad_y_pred)
grad_h_relu = grad_y_pred.mm(w2.t())
grad_h = grad_h_relu.clone()
grad_h[h < 0] = 0
grad_w1 = x.t().mm(grad_h)

# Update weights using gradient descent
w1 -= learning_rate * grad_w1
w2 -= learning_rate * grad_w2
```

Q: Can you decrease the loss down ? Do whatever you like and plot the loss-epoch graph.

End.