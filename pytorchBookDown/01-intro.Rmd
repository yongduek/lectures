# Introduction {#intro}




The codes here are from [https://pytorch.org/tutorials/beginner/pytorch_with_examples.html](Learning Pytorch with Examples)

The python codes are modified just a little bit to fit in the book format; instead of printing out values, their 2d plots are drawn.

Make sure that all the required modules & packages are installed in your machine.


## Warm-up numpy

Let's make a small network model with `numpy`.

```{python}
import sys
import numpy as np
import matplotlib.pyplot as plt

# N is batch size; D_in is input dimension;
# H is hidden dimension; D_out is output dimension.
N, D_in, H, D_out = 64, 1000, 100, 10

# Create random input and output data
x = np.random.randn(N, D_in)
y = np.random.randn(N, D_out)

# Randomly initialize weights
w1 = np.random.randn(D_in, H)
w2 = np.random.randn(H, D_out)

learning_rate = 1e-6
niter = 20
res = []
for t in range(niter):
    # Forward pass: compute predicted y
    h = x.dot(w1)
    h_relu = np.maximum(h, 0)
    y_pred = h_relu.dot(w2)

    # Compute and print loss
    loss = np.square(y_pred - y).sum()
    #print(t, loss)
    res.append (loss)
    
    # Backprop to compute gradients of w1 and w2 with respect to loss
    grad_y_pred = 2.0 * (y_pred - y)
    grad_w2 = h_relu.T.dot(grad_y_pred)
    grad_h_relu = grad_y_pred.dot(w2.T)
    grad_h = grad_h_relu.copy()
    grad_h[h < 0] = 0
    grad_w1 = x.T.dot(grad_h)

    # Update weights
    w1 -= learning_rate * grad_w1
    w2 -= learning_rate * grad_w2
#
plt.plot (range(niter), res, '-o')
plt.ylabel ('loss')
plt.xlabel ('epoch')
plt.show()
```

So, what is happening here?

- The network will have 64 input items: `N=64`. This is called a *batch* or *mini_batch* because it is small.

- The network's input is of dimension 1000: `D_in=1000`
- The network has one hidden layer of size 100: `H=100`
- The network's output is of dimension 10: `D_out = 10`
```{python}
# N is batch size; D_in is input dimension;
# H is hidden dimension; D_out is output dimension.
N, D_in, H, D_out = 64, 1000, 100, 10
```

Then, the input matrix $x$ of size `N x D_in` is created randomly since here we do not want to be bothered how to get it from some source. The output matrix $y$ corresponding to the input $x$ is randomly created too. We have N=64 correspondences, and we want to find a function $f$ that maps $y_i = f(x_i)$ for $i=1,...,N$.
```{python}
# Create random input and output data
x = np.random.randn(N, D_in)
y = np.random.randn(N, D_out)
```

Now we create two networks $w_1$ and $w_2$ that transfers from input to the hidden layer and from the hidden layer to the output.

```{python}
# Randomly initialize weights
w1 = np.random.randn(D_in, H)
w2 = np.random.randn(H, D_out)
```

The function $f$ that we want to seek is parametrized by several steps:

1. linear mapping, $w_1$, a matrix of size $1000\times100$ 
2. relu: *Rectifier Linear Unit*
3. linear mapping, $w_2$, another weight matrix of size $100\times10$

The procedure is encoded in python codes with `numpy` functions as follows

```{python eval=FALSE}
# Forward pass: compute predicted y
h = x.dot(w1)
h_relu = np.maximum(h, 0)
y_pred = h_relu.dot(w2)
```

Q: what is the meaning of them: `x.dot(w1)`, `np.maximum(h,0)`, `h_relu.dot(w2)` ?