
## With Pytorch Optim for optimization

Now let's see how we can automate the parameter update procedure using another pytorch module [`torch.optim`](https://pytorch.org/docs/stable/optim.html).

```{python}
import torch

# N is batch size; D_in is input dimension;
# H is hidden dimension; D_out is output dimension.
N, D_in, H, D_out = 64, 1000, 100, 10

# Create random Tensors to hold inputs and outputs
x = torch.randn(N, D_in)
y = torch.randn(N, D_out)

# Use the nn package to define our model and loss function.
model = torch.nn.Sequential(
    torch.nn.Linear(D_in, H),
    torch.nn.ReLU(),
    torch.nn.Linear(H, D_out),
)
loss_fn = torch.nn.MSELoss(reduction='sum')

# Use the optim package to define an Optimizer that will update the weights of
# the model for us. Here we will use Adam; the optim package contains many other
# optimization algoriths. The first argument to the Adam constructor tells the
# optimizer which Tensors it should update.
learning_rate = 1e-4
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
#
res = []
epochs = 500
for t in range(500):
    # Forward pass: compute predicted y by passing x to the model.
    y_pred = model(x)

    # Compute and print loss.
    loss = loss_fn(y_pred, y)
    res.append (loss.item())
    
    # Before the backward pass, use the optimizer object to zero all of the
    # gradients for the variables it will update (which are the learnable
    # weights of the model). This is because by default, gradients are
    # accumulated in buffers( i.e, not overwritten) whenever .backward()
    # is called. Checkout docs of torch.autograd.backward for more details.
    optimizer.zero_grad()

    # Backward pass: compute gradient of the loss with respect to model
    # parameters
    loss.backward()

    # Calling the step function on an Optimizer makes an update to its
    # parameters
    optimizer.step()
  #
```

Use the optim package [`torch.optim`](https://pytorch.org/docs/stable/optim.html) to define an Optimizer that will update the weights of
the model for us. Here we choose to use something called `torch.optim.Adam`; the `optim` package contains many other optimization algorithms. The first argument to the Adam constructor tells the
optimizer which Tensors it should update after automatic gradient computation.
```
learning_rate = 1e-4
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
```

In the learning loop, you must clear the gradient buffer:
```
optimizer.zero_grad()
```
Q. How did we clear the gradient buffer when we were not using `optim`?

Then we compute the gradient of the loss with respect to the model parameters.
```
loss.backward()
```

Finally, we update the model parameters by asking `optimizer` to do ti.
```
optimizer.step()
```

Now the learning result.
```{python}
import matplotlib.pyplot as plt

print ('loss = {} after epochs {}'.format(loss, epochs))
fig, ax = plt.subplots();
ax.plot (range(1,1+epochs), res, '-')
fig.suptitle ('loss-epoch curve with NN module')
plt.show()
```