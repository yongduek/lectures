{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple MDP Model\n",
    "$$\n",
    "T \\longleftrightarrow A \\longleftrightarrow B \\longleftrightarrow C \\overset{1}{\\longleftrightarrow} T\n",
    "$$\n",
    "\n",
    "Policy Evaluation (Prediction)  [See 4.1. Policy Evaluation, Sutton's book](http://incompleteideas.net/book/first/ebook/node41.html)\n",
    "\n",
    "\\begin{align}\n",
    "    V^\\pi & = \\mathbb{E}_\\pi \\big\\{   \n",
    "                    r_{t+1} + \\gamma r_{t+2} + ... | s_t = s\n",
    "                \\big\\}  \\\\\n",
    "          & =\n",
    "             \\mathbb{E}_\\pi\\big\\{ r_{t+1} + \\gamma V^\\pi(s_{t+1})|s_t = s \\big\\} \\\\\n",
    "          & =\n",
    "              \\sum_a \\pi (s,a) \\sum_{s'}  \\mathcal{P}_{ss'}^a\\big[ \\mathcal{R}_{ss'}^a + \\gamma V^\\pi(s') \\big]\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MC method for policy evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Envornment:  # MDP\n",
    "    \n",
    "    def __init__(self, state_name_string='ABCDE'):\n",
    "        rng = np.random.default_rng(2021)\n",
    "        self.stat_names = list(state_name_string)\n",
    "        self.stats = np.arange(len(self.stat_names))\n",
    "        self.current = self.stats[rng.integers(0, len(self.stats))]\n",
    "        \n",
    "        self.reset()\n",
    "        \n",
    "    def step(self, action):  # action = [0, 1] left or right\n",
    "        done = False\n",
    "        if action == 0:\n",
    "            self.current -= 1\n",
    "            if self.current < 0:\n",
    "                self.current = 0\n",
    "                done = True\n",
    "            reward = 0\n",
    "            \n",
    "        elif action == 1: # right\n",
    "            self.current += 1\n",
    "            if self.current >= len(self.stats):\n",
    "                self.current -= 1\n",
    "                done = True\n",
    "                reward = 1\n",
    "            else:\n",
    "                reward = 0\n",
    "\n",
    "        info = {}  # just to be gym\n",
    "        \n",
    "        return self.current, reward, done, info\n",
    "    \n",
    "    def reset(self,):\n",
    "        self.current = self.stats[rng.integers(0, len(self.stats))]\n",
    "        return self.current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random action policy\n",
    "rng = np.random.default_rng(2021)\n",
    "\n",
    "def policy(state):\n",
    "    return rng.integers(0, 2)\n",
    "\n",
    "action_names = ['left', 'right']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init state:  B\n",
      "right 2 0 False  :  2 C 0 False\n",
      "left 1 0 False  :  1 B 0 False\n",
      "left 0 0 False  :  0 A 0 False\n",
      "left 0 0 True  :  0 Terminated. 0 True\n"
     ]
    }
   ],
   "source": [
    "env = Envornment('ABC')\n",
    "\n",
    "state = env.reset()\n",
    "print('init state: ', env.stat_names[state])\n",
    "\n",
    "done = False\n",
    "while done == False:\n",
    "    action = policy(state)\n",
    "    s, r, done, _ = env.step(action)\n",
    "    print(action_names[action], s,r, done, end='  :  ')\n",
    "    if done:    \n",
    "        print(s, 'Terminated.', r, done)    \n",
    "    else:\n",
    "        print(s, env.stat_names[s], r, done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env, verbose=True):\n",
    "    sar = []\n",
    "    state = env.reset()\n",
    "    if verbose:\n",
    "        print('init state: ', env.stat_names[state])\n",
    "\n",
    "    done = False\n",
    "    while done == False:\n",
    "        action = policy(state)\n",
    "        s, r, done, _ = env.step(action)\n",
    "        \n",
    "        sar.append((state, action, r))\n",
    "        state = s  \n",
    "        \n",
    "        if verbose:\n",
    "            print(action_names[action], s,r, done, end='  :  ')\n",
    "            if done:    \n",
    "                print(s, 'Terminated.', r, done)    \n",
    "            else:\n",
    "                print(s, env.stat_names[s], r, done)\n",
    "    #\n",
    "    return sar       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([19., 67., 88.]),\n",
       " array([ 77., 114., 102.]),\n",
       " array([0.24675325, 0.5877193 , 0.8627451 ]))"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First visit Monte-Carlo Value policy evaluation\n",
    "\n",
    "env = Envornment('ABC')\n",
    "N = np.zeros(len(env.stats))  # number of visit\n",
    "V = np.zeros_like(N)          # value function\n",
    "\n",
    "for _ in range(100):\n",
    "    sar = run_episode(env, verbose=False)\n",
    "\n",
    "    sar = np.array(sar)\n",
    "    for i in range(sar.shape[0]):  # episode length\n",
    "        start_state = sar[i, 0]\n",
    "        G = sar[i:,-1].sum()  # total sum of rewards until termination\n",
    "        N[start_state] += 1\n",
    "        V[start_state] += G\n",
    "\n",
    "V, N, V/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First visit Monte-Carlo Value policy evaluation\n",
    "def policy_eval(niter=100):\n",
    "    env = Envornment('ABC')\n",
    "    N = np.zeros(len(env.stats))  # number of visit\n",
    "    V = np.zeros_like(N)          # value function\n",
    "\n",
    "    for _ in range(niter):\n",
    "        sar = run_episode(env, verbose=False)\n",
    "\n",
    "        sar = np.array(sar)\n",
    "        for i in range(sar.shape[0]):  # episode length\n",
    "            start_state = sar[i, 0]\n",
    "            G = sar[i:,-1].sum()  # total sum of rewards until termination\n",
    "            N[start_state] += 1\n",
    "            V[start_state] += G\n",
    "    return V/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.35643564, 0.62318841, 0.80733945])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_eval(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.24053541, 0.49058718, 0.74494212])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_eval(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.24843128, 0.49856658, 0.74854524])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_eval(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.24983966, 0.5026176 , 0.75475681])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_eval(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.2509998 , 0.50092254, 0.75066788])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_eval(1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.24910225, 0.49848645, 0.74886199])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_eval(1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.25019204, 0.5002766 , 0.75026337])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_eval(10000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exact solution to the policy evalution problem of the MDP Model\n",
    "$$\n",
    "T \\longleftrightarrow A \\longleftrightarrow B \\longleftrightarrow C \\overset{1}{\\longleftrightarrow} T\n",
    "$$\n",
    "\n",
    "Policy Evaluation (Prediction)  [See 4.1. Policy Evaluation, Sutton's book](http://incompleteideas.net/book/first/ebook/node41.html)\n",
    "\n",
    "\\begin{align}\n",
    "    V^\\pi & = \\mathbb{E}_\\pi \\big\\{   \n",
    "                    r_{t+1} + \\gamma r_{t+2} + ... | s_t = s\n",
    "                \\big\\}  \\\\\n",
    "          & =\n",
    "             \\mathbb{E}_\\pi\\big\\{ r_{t+1} + \\gamma V^\\pi(s_{t+1})|s_t = s \\big\\} \\\\\n",
    "          & =\n",
    "              \\sum_a \\pi (s,a) \\sum_{s'}  \\mathcal{P}_{ss'}^a\\big[ \\mathcal{R}_{ss'}^a + \\gamma V^\\pi(s') \\big]\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With random action policy,\n",
    "\n",
    "V(A) = A = 1/2 (0 + 0 + B)\n",
    "V(B) = B = 1/2 (0 + A + 0 + C)\n",
    "V(C) = C = 1/2 (0 + B + 1)\n",
    "\n",
    "V = [ [0, .5, 0], [.5, 0, .5], [0, .5, 0] ] V + [0, 0, .5]\n",
    "\n",
    "V = inv(I - M) [0, 0, .5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I = np.eye(3); I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[0, .5, 0], [.5, 0, .5], [0, .5, 0]])\n",
    "\n",
    "b = np.array([0,0,.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.5, 1. , 0.5],\n",
       "       [1. , 2. , 1. ],\n",
       "       [0.5, 1. , 1.5]])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.inv(I-A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.25, 0.5 , 0.75])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.inv(I-A) @ b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exact solution to the policy evalution problem\n",
    "$$\n",
    "T \\longleftrightarrow A \\longleftrightarrow \\ldots \\overset{0}{\\longleftrightarrow} E \\overset{1}{\\longleftrightarrow} T\n",
    "$$\n",
    "\n",
    "Policy Evaluation (Prediction)  [See 4.1. Policy Evaluation, Sutton's book](http://incompleteideas.net/book/first/ebook/node41.html)\n",
    "\n",
    "\\begin{align}\n",
    "    V^\\pi & = \\mathbb{E}_\\pi \\big\\{   \n",
    "                    r_{t+1} + \\gamma r_{t+2} + ... | s_t = s\n",
    "                \\big\\}  \\\\\n",
    "          & =\n",
    "             \\mathbb{E}_\\pi\\big\\{ r_{t+1} + \\gamma V^\\pi(s_{t+1})|s_t = s \\big\\} \\\\\n",
    "          & =\n",
    "              \\sum_a \\pi (s,a) \\sum_{s'}  \\mathcal{P}_{ss'}^a\\big[ \\mathcal{R}_{ss'}^a + \\gamma V^\\pi(s') \\big]\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "I = np.eye(5)\n",
    "A = np.array([ [0, .5, 0, 0, 0],\n",
    "               [.5, 0, .5, 0, 0],\n",
    "               [0, .5, 0, .5, 0],\n",
    "               [0, 0, .5, 0, .5],\n",
    "               [0, 0, 0, .5, 0]])\n",
    "b = np.array([0, 0, 0, 0, .5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.16666667, 0.33333333, 0.5       , 0.66666667, 0.83333333])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.inv(I - A) @ b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
