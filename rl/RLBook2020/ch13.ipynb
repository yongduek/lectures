{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "\r\n",
    "import numpy as np\r\n",
    "import matplotlib\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from tqdm import tqdm\r\n",
    "\r\n",
    "def true_value(p):\r\n",
    "    \"\"\" True value of the first state\r\n",
    "    Args:\r\n",
    "        p (float): probability of the action 'right'.\r\n",
    "    Returns:\r\n",
    "        True value of the first state.\r\n",
    "        The expression is obtained by manually solving the easy linear system\r\n",
    "        of Bellman equations using known dynamics.\r\n",
    "    \"\"\"\r\n",
    "    return (2 * p - 4) / (p * (1 - p))\r\n",
    "\r\n",
    "class ShortCorridorEnv:\r\n",
    "    \"\"\"\r\n",
    "    Short corridor environment, see Example 13.1\r\n",
    "    \"\"\"\r\n",
    "    def __init__(self):\r\n",
    "        self.reset()\r\n",
    "\r\n",
    "    def reset(self):\r\n",
    "        self.state = 0\r\n",
    "        return self.state \r\n",
    "\r\n",
    "    def step(self, go_right):  # follow gym interface\r\n",
    "        \"\"\"\r\n",
    "        Args:\r\n",
    "            go_right (bool): chosen action\r\n",
    "        Returns:\r\n",
    "            tuple of (state, reward, episode terminated?)\r\n",
    "        \"\"\"\r\n",
    "        if self.state == 0 or self.state == 2:\r\n",
    "            if go_right:\r\n",
    "                self.state += 1\r\n",
    "            else:\r\n",
    "                self.state = max(0, self.state - 1)\r\n",
    "        else:\r\n",
    "            if go_right:\r\n",
    "                self.state -= 1\r\n",
    "            else:\r\n",
    "                self.state += 1\r\n",
    "\r\n",
    "        info = {}\r\n",
    "        if self.state == 3:\r\n",
    "            # terminal state\r\n",
    "            return self.state, 0, True, info \r\n",
    "        else:\r\n",
    "            return self.state, -1, False, info\r\n",
    "#"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The agent in Example 13.1 does not use the state information to estimate policy or value function\r\n",
    "- The agent maintains only one probability for every state. (This can be thought of as a constraint for the problem.)\r\n",
    "- The policy is defined to be \r\n",
    "$$\r\n",
    "    \\pi = \\theta_0 x_0 + \\theta_1 x_1  \\in \\mathbb{R}^2   \\quad\\text{where}\\quad    x_0 = [1,0], x_1 = [0, 1]\r\n",
    "$$\r\n",
    "\r\n",
    "- It will perform well if a policy probability was defined for each state.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# REINFORCE algorithm for the problem in Example 13.1\r\n",
    "def softmax(a):\r\n",
    "    exp = np.exp(a)\r\n",
    "    sm = exp / exp.sum()\r\n",
    "    return sm \r\n",
    "\r\n",
    "class ReinforceAgent:\r\n",
    "    def __init__(self, alpha, gamma):\r\n",
    "        self.alpha = alpha \r\n",
    "        self.gamma = gamma \r\n",
    "        self.theta = np.array([-1, 1])   # initial parameter for pi\r\n",
    "        self.x = np.array([[1, 0], [0, 1]])  # feature vector, constant\r\n",
    "        self.pi = None  # initial pi\r\n",
    "\r\n",
    "        # policy is defined to be\r\n",
    "    def get_action(self, ):  # state information is not required. \r\n",
    "        self.pi = softmax(np.dot(self.theta, self.x))  # PMF\r\n",
    "        if np.random.uniform() < self.pi[0]:\r\n",
    "            return 0  # go left\r\n",
    "        else:\r\n",
    "            return 1\r\n",
    "\r\n",
    "    def learn(self, rewards, actions):\r\n",
    "        G = 0\r\n",
    "        Ghist = []\r\n",
    "        for r in rewards[::-1]:\r\n",
    "            G += r + self.gamma * G \r\n",
    "            Ghist.insert(0, G)\r\n",
    "        Ghist = np.array(Ghist)\r\n",
    "        Ghist = (Ghist - Ghist.mean()) / Ghist.std()  # data normalization\r\n",
    "\r\n",
    "        delta = 0\r\n",
    "        \r\n",
    "        for t, (G, a) in enumerate(zip(Ghist, actions)):\r\n",
    "            grad_log_pi = self.x[:,a] - self.x @ self.pi\r\n",
    "            d = self.alpha * np.power(self.gamma, t) * G * grad_log_pi\r\n",
    "\r\n",
    "            self.theta += d \r\n",
    "    #\r\n",
    "\r\n",
    "        pass"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def trial(num_episodes, agent_generator):\r\n",
    "    env = ShortCorridorEnv()\r\n",
    "    agent = agent_generator()\r\n",
    "\r\n",
    "    rewards = np.zeros(num_episodes)\r\n",
    "    for episode_idx in range(num_episodes):\r\n",
    "        reward_hist, action_hist = [], []\r\n",
    "\r\n",
    "        env.reset()\r\n",
    "        while True:\r\n",
    "            go_right = agent.get_action()\r\n",
    "            state, reward, episode_end, info = env.step(go_right)\r\n",
    "            reward_hist.append(reward)\r\n",
    "            action_hist.append(go_right)\r\n",
    "\r\n",
    "            if episode_end:        \r\n",
    "                break\r\n",
    "\r\n",
    "        reward_hist = np.array(reward_hist)\r\n",
    "        agent.learn(reward_hist, action_hist)  # update agent parameters\r\n",
    "        rewards[episode_idx] = sum(reward_hist)\r\n",
    "\r\n",
    "    return rewards\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def figure_13_1():\r\n",
    "    num_trials = 100\r\n",
    "    num_episodes = 1000\r\n",
    "    gamma = 1\r\n",
    "    agent_generators = [lambda : ReinforceAgent(alpha=2e-4, gamma=gamma),\r\n",
    "                        lambda : ReinforceAgent(alpha=2e-5, gamma=gamma),\r\n",
    "                        lambda : ReinforceAgent(alpha=2e-3, gamma=gamma)]\r\n",
    "    labels = ['alpha = 2e-4',\r\n",
    "              'alpha = 2e-5',\r\n",
    "              'alpha = 2e-3']\r\n",
    "\r\n",
    "    rewards = np.zeros((len(agent_generators), num_trials, num_episodes))\r\n",
    "\r\n",
    "    for agent_index, agent_generator in enumerate(agent_generators):\r\n",
    "        for i in tqdm(range(num_trials)):\r\n",
    "            reward = trial(num_episodes, agent_generator)\r\n",
    "            rewards[agent_index, i, :] = reward\r\n",
    "\r\n",
    "    plt.plot(np.arange(num_episodes) + 1, -11.6 * np.ones(num_episodes), ls='dashed', color='red', label='-11.6')\r\n",
    "    for i, label in enumerate(labels):\r\n",
    "        plt.plot(np.arange(num_episodes) + 1, rewards[i].mean(axis=0), label=label)\r\n",
    "    plt.ylabel('total reward on episode')\r\n",
    "    plt.xlabel('episode')\r\n",
    "    plt.legend(loc='lower right')\r\n",
    "\r\n",
    "    plt.savefig('./images/figure_13_1.png')\r\n",
    "    plt.close()\r\n",
    "#\r\n",
    "\r\n",
    "figure_13_1()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [01:05<00:00,  1.54it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [02:44<00:00,  1.65s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [02:02<00:00,  1.22s/it]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def figure_13_2():\r\n",
    "    num_trials = 100\r\n",
    "    num_episodes = 1000\r\n",
    "    alpha = 2e-4\r\n",
    "    gamma = 1\r\n",
    "    agent_generators = [lambda : ReinforceAgent(alpha=alpha, gamma=gamma),\r\n",
    "                        lambda : ReinforceBaselineAgent(alpha=alpha*10, gamma=gamma, alpha_w=alpha*100)]\r\n",
    "    labels = ['Reinforce without baseline',\r\n",
    "              'Reinforce with baseline']\r\n",
    "\r\n",
    "    rewards = np.zeros((len(agent_generators), num_trials, num_episodes))\r\n",
    "\r\n",
    "    for agent_index, agent_generator in enumerate(agent_generators):\r\n",
    "        for i in tqdm(range(num_trials)):\r\n",
    "            reward = trial(num_episodes, agent_generator)\r\n",
    "            rewards[agent_index, i, :] = reward\r\n",
    "\r\n",
    "    plt.plot(np.arange(num_episodes) + 1, -11.6 * np.ones(num_episodes), ls='dashed', color='red', label='-11.6')\r\n",
    "    for i, label in enumerate(labels):\r\n",
    "        plt.plot(np.arange(num_episodes) + 1, rewards[i].mean(axis=0), label=label)\r\n",
    "    plt.ylabel('total reward on episode')\r\n",
    "    plt.xlabel('episode')\r\n",
    "    plt.legend(loc='lower right')\r\n",
    "\r\n",
    "    plt.savefig('./images/figure_13_2.png')\r\n",
    "    plt.close()\r\n",
    "#\r\n",
    "figure_13_2()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [01:04<00:00,  1.55it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:52<00:00,  1.89it/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "interpreter": {
   "hash": "5c2412a7f059a44a3ebbda245f298c96603c0f68fdadd5589b1fcd57ac7e1e28"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}