{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# Copyright (C)                                                       #\n",
    "# 2018 Sergii Bondariev (sergeybondarev@gmail.com)                    #\n",
    "# 2018 Shangtong Zhang(zhangshangtong.cpp@gmail.com)                  #\n",
    "# Permission given to modify the code as long as you keep this        #\n",
    "# declaration at the top                                              #\n",
    "#######################################################################\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "def true_value(p):\n",
    "    \"\"\" True value of the first state\n",
    "    Args:\n",
    "        p (float): probability of the action 'right'.\n",
    "    Returns:\n",
    "        True value of the first state.\n",
    "        The expression is obtained by manually solving the easy linear system\n",
    "        of Bellman equations using known dynamics.\n",
    "    \"\"\"\n",
    "    return (2 * p - 4) / (p * (1 - p))\n",
    "\n",
    "class ShortCorridor:\n",
    "    \"\"\"\n",
    "    Short corridor environment, see Example 13.1\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = 0\n",
    "\n",
    "    def step(self, go_right):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            go_right (bool): chosen action\n",
    "        Returns:\n",
    "            tuple of (reward, episode terminated?)\n",
    "        \"\"\"\n",
    "        if self.state == 0 or self.state == 2:\n",
    "            if go_right:\n",
    "                self.state += 1\n",
    "            else:\n",
    "                self.state = max(0, self.state - 1)\n",
    "        else:\n",
    "            if go_right:\n",
    "                self.state -= 1\n",
    "            else:\n",
    "                self.state += 1\n",
    "\n",
    "        if self.state == 3:\n",
    "            # terminal state\n",
    "            return 0, True\n",
    "        else:\n",
    "            return -1, False\n",
    "\n",
    "def softmax(x):\n",
    "    t = np.exp(x - np.max(x))\n",
    "    return t / np.sum(t)\n",
    "\n",
    "class ReinforceAgent:\n",
    "    \"\"\"\n",
    "    ReinforceAgent that follows algorithm\n",
    "    'REINFORNCE Monte-Carlo Policy-Gradient Control (episodic)'\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha, gamma):\n",
    "        # set values such that initial conditions correspond to left-epsilon greedy\n",
    "        self.theta = np.array([-1.47, 1.47])\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        # first column - left, second - right\n",
    "        self.x = np.array([[0, 1],\n",
    "                           [1, 0]])\n",
    "        self.rewards = []\n",
    "        self.actions = []\n",
    "\n",
    "    def get_pi(self):\n",
    "        h = np.dot(self.theta, self.x)\n",
    "        t = np.exp(h - np.max(h))\n",
    "        pmf = t / np.sum(t)\n",
    "        # never become deterministic,\n",
    "        # guarantees episode finish\n",
    "        imin = np.argmin(pmf)\n",
    "        epsilon = 0.05\n",
    "\n",
    "        if pmf[imin] < epsilon:\n",
    "            pmf[:] = 1 - epsilon\n",
    "            pmf[imin] = epsilon\n",
    "\n",
    "        return pmf\n",
    "\n",
    "    def get_p_right(self):\n",
    "        return self.get_pi()[1]\n",
    "\n",
    "    def choose_action(self, reward):\n",
    "        if reward is not None:\n",
    "            self.rewards.append(reward)\n",
    "\n",
    "        pmf = self.get_pi()\n",
    "        go_right = np.random.uniform() <= pmf[1]\n",
    "        self.actions.append(go_right)\n",
    "\n",
    "        return go_right\n",
    "\n",
    "    def episode_end(self, last_reward):\n",
    "        self.rewards.append(last_reward)\n",
    "\n",
    "        # learn theta\n",
    "        G = np.zeros(len(self.rewards))\n",
    "        G[-1] = self.rewards[-1]\n",
    "\n",
    "        for i in range(2, len(G) + 1):\n",
    "            G[-i] = self.gamma * G[-i + 1] + self.rewards[-i]\n",
    "\n",
    "        gamma_pow = 1\n",
    "\n",
    "        for i in range(len(G)):\n",
    "            j = 1 if self.actions[i] else 0\n",
    "            pmf = self.get_pi()\n",
    "            grad_ln_pi = self.x[:, j] - np.dot(self.x, pmf)\n",
    "            update = self.alpha * gamma_pow * G[i] * grad_ln_pi\n",
    "\n",
    "            self.theta += update\n",
    "            gamma_pow *= self.gamma\n",
    "\n",
    "        self.rewards = []\n",
    "        self.actions = []\n",
    "\n",
    "class ReinforceBaselineAgent(ReinforceAgent):\n",
    "    def __init__(self, alpha, gamma, alpha_w):\n",
    "        super(ReinforceBaselineAgent, self).__init__(alpha, gamma)\n",
    "        self.alpha_w = alpha_w\n",
    "        self.w = 0\n",
    "\n",
    "    def episode_end(self, last_reward):\n",
    "        self.rewards.append(last_reward)\n",
    "\n",
    "        # learn theta\n",
    "        G = np.zeros(len(self.rewards))\n",
    "        G[-1] = self.rewards[-1]\n",
    "\n",
    "        for i in range(2, len(G) + 1):\n",
    "            G[-i] = self.gamma * G[-i + 1] + self.rewards[-i]\n",
    "\n",
    "        gamma_pow = 1\n",
    "\n",
    "        for i in range(len(G)):\n",
    "            self.w += self.alpha_w * gamma_pow * (G[i] - self.w)\n",
    "\n",
    "            j = 1 if self.actions[i] else 0\n",
    "            pmf = self.get_pi()\n",
    "            grad_ln_pi = self.x[:, j] - np.dot(self.x, pmf)\n",
    "            update = self.alpha * gamma_pow * (G[i] - self.w) * grad_ln_pi\n",
    "\n",
    "            self.theta += update\n",
    "            gamma_pow *= self.gamma\n",
    "\n",
    "        self.rewards = []\n",
    "        self.actions = []\n",
    "\n",
    "def trial(num_episodes, agent_generator):\n",
    "    env = ShortCorridor()\n",
    "    agent = agent_generator()\n",
    "\n",
    "    rewards = np.zeros(num_episodes)\n",
    "    for episode_idx in range(num_episodes):\n",
    "        rewards_sum = 0\n",
    "        reward = None\n",
    "        env.reset()\n",
    "\n",
    "        while True:\n",
    "            go_right = agent.choose_action(reward)\n",
    "            reward, episode_end = env.step(go_right)\n",
    "            rewards_sum += reward\n",
    "\n",
    "            if episode_end:\n",
    "                agent.episode_end(reward)\n",
    "                break\n",
    "\n",
    "        rewards[episode_idx] = rewards_sum\n",
    "\n",
    "    return rewards\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_13_1():\n",
    "    epsilon = 0.05\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "    # Plot a graph\n",
    "    p = np.linspace(0.01, 0.99, 100)\n",
    "    y = true_value(p)\n",
    "    ax.plot(p, y, color='red')\n",
    "\n",
    "    # Find a maximum point, can also be done analytically by taking a derivative\n",
    "    imax = np.argmax(y)\n",
    "    pmax = p[imax]\n",
    "    ymax = y[imax]\n",
    "    ax.plot(pmax, ymax, color='green', marker=\"*\", label=\"optimal point: f({0:.2f}) = {1:.2f}\".format(pmax, ymax))\n",
    "\n",
    "    # Plot points of two epsilon-greedy policies\n",
    "    ax.plot(epsilon, true_value(epsilon), color='magenta', marker=\"o\", label=\"epsilon-greedy left\")\n",
    "    ax.plot(1 - epsilon, true_value(1 - epsilon), color='blue', marker=\"o\", label=\"epsilon-greedy right\")\n",
    "\n",
    "    ax.set_ylabel(\"Value of the first state\")\n",
    "    ax.set_xlabel(\"Probability of the action 'right'\")\n",
    "    ax.set_title(\"Short corridor with switched actions\")\n",
    "    ax.set_ylim(ymin=-105.0, ymax=5)\n",
    "    ax.legend()\n",
    "\n",
    "    plt.savefig('./images/example_13_1.png')\n",
    "    plt.close()\n",
    "#\n",
    "example_13_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [01:05<00:00,  1.54it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [02:44<00:00,  1.65s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [02:02<00:00,  1.22s/it]\n"
     ]
    }
   ],
   "source": [
    "def figure_13_1():\n",
    "    num_trials = 100\n",
    "    num_episodes = 1000\n",
    "    gamma = 1\n",
    "    agent_generators = [lambda : ReinforceAgent(alpha=2e-4, gamma=gamma),\n",
    "                        lambda : ReinforceAgent(alpha=2e-5, gamma=gamma),\n",
    "                        lambda : ReinforceAgent(alpha=2e-3, gamma=gamma)]\n",
    "    labels = ['alpha = 2e-4',\n",
    "              'alpha = 2e-5',\n",
    "              'alpha = 2e-3']\n",
    "\n",
    "    rewards = np.zeros((len(agent_generators), num_trials, num_episodes))\n",
    "\n",
    "    for agent_index, agent_generator in enumerate(agent_generators):\n",
    "        for i in tqdm(range(num_trials)):\n",
    "            reward = trial(num_episodes, agent_generator)\n",
    "            rewards[agent_index, i, :] = reward\n",
    "\n",
    "    plt.plot(np.arange(num_episodes) + 1, -11.6 * np.ones(num_episodes), ls='dashed', color='red', label='-11.6')\n",
    "    for i, label in enumerate(labels):\n",
    "        plt.plot(np.arange(num_episodes) + 1, rewards[i].mean(axis=0), label=label)\n",
    "    plt.ylabel('total reward on episode')\n",
    "    plt.xlabel('episode')\n",
    "    plt.legend(loc='lower right')\n",
    "\n",
    "    plt.savefig('./images/figure_13_1.png')\n",
    "    plt.close()\n",
    "#\n",
    "\n",
    "figure_13_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [01:04<00:00,  1.55it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:52<00:00,  1.89it/s]\n"
     ]
    }
   ],
   "source": [
    "def figure_13_2():\n",
    "    num_trials = 100\n",
    "    num_episodes = 1000\n",
    "    alpha = 2e-4\n",
    "    gamma = 1\n",
    "    agent_generators = [lambda : ReinforceAgent(alpha=alpha, gamma=gamma),\n",
    "                        lambda : ReinforceBaselineAgent(alpha=alpha*10, gamma=gamma, alpha_w=alpha*100)]\n",
    "    labels = ['Reinforce without baseline',\n",
    "              'Reinforce with baseline']\n",
    "\n",
    "    rewards = np.zeros((len(agent_generators), num_trials, num_episodes))\n",
    "\n",
    "    for agent_index, agent_generator in enumerate(agent_generators):\n",
    "        for i in tqdm(range(num_trials)):\n",
    "            reward = trial(num_episodes, agent_generator)\n",
    "            rewards[agent_index, i, :] = reward\n",
    "\n",
    "    plt.plot(np.arange(num_episodes) + 1, -11.6 * np.ones(num_episodes), ls='dashed', color='red', label='-11.6')\n",
    "    for i, label in enumerate(labels):\n",
    "        plt.plot(np.arange(num_episodes) + 1, rewards[i].mean(axis=0), label=label)\n",
    "    plt.ylabel('total reward on episode')\n",
    "    plt.xlabel('episode')\n",
    "    plt.legend(loc='lower right')\n",
    "\n",
    "    plt.savefig('./images/figure_13_2.png')\n",
    "    plt.close()\n",
    "#\n",
    "figure_13_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
