{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# TicTacToe Game & Tabular Q Learning"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt \r\n",
    "\r\n",
    "rng = np.random.default_rng(2021)  # random number generator"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "source": [
    "class EnvTTT():\r\n",
    "    def __init__(self,):\r\n",
    "        self.reset()\r\n",
    "        \r\n",
    "    def reset(self,):\r\n",
    "        self.board = '---------'\r\n",
    "        self.state = self.board\r\n",
    "        self.winner = None \r\n",
    "\r\n",
    "        return self.state \r\n",
    "        \r\n",
    "    def step(self, action, ox):\r\n",
    "        \"\"\" put X/O at the location specified by action\r\n",
    "            action: a number 0 ~ 9 \r\n",
    "            return: state, reward, done, info \"\"\"\r\n",
    "        done = False \r\n",
    "        reward = 0\r\n",
    "        info = {}\r\n",
    "        if self.state[action] != '-':\r\n",
    "            done = True \r\n",
    "            info = {'state' : self.state, 'code': 'bad action: occupied' }\r\n",
    "        else:\r\n",
    "            self.state = self.state[:action] + ox + self.state[action+1:]\r\n",
    "\r\n",
    "        if self.game_over() == True:\r\n",
    "            if self.winner == 'D':\r\n",
    "                reward = 0\r\n",
    "            else:\r\n",
    "                reward = 1\r\n",
    "            done = True\r\n",
    "\r\n",
    "        return self.state, reward, done, info \r\n",
    "\r\n",
    "    def game_over(self,):            \r\n",
    "        # Each list corresponds to the values to check to see if a winner is there\r\n",
    "        checks = [[0, 1, 2], [3, 4, 5], [6, 7, 8], [0, 3, 6], [1, 4, 7] , [2, 5, 8], [0, 4, 8], [2, 4, 6]]\r\n",
    "\r\n",
    "        for check in checks:\r\n",
    "            # Check to see if the strings have a winner\r\n",
    "            test = self.state[check[0]] + self.state[check[1]] + self.state[check[2]]\r\n",
    "            if test == 'XXX':\r\n",
    "                self.winner = 'X'\r\n",
    "                # print('>>>', test, self.winner)\r\n",
    "                return True\r\n",
    "            elif test == 'OOO':\r\n",
    "                self.winner = 'O'\r\n",
    "                # print('>>>', test, self.winner)\r\n",
    "                return True\r\n",
    "\r\n",
    "        if '-' not in self.state:\r\n",
    "            \"\"\" draw \"\"\"\r\n",
    "            self.winner = 'D'  # draw\r\n",
    "            return True \r\n",
    "        return False\r\n",
    "\r\n",
    "    def render(self,):\r\n",
    "        for i in range(0, 9, 3):\r\n",
    "            print(self.state[i:i+3])\r\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "source": [
    "class QAgent():\r\n",
    "    def __init__(self, ox):\r\n",
    "        self.ox = ox \r\n",
    "        # { state: [q1, q2, ..., q9] } where q_n = -inf if action n is impossible.\r\n",
    "        # use self.get_Qvalues(state) interface. Do not try to access self.Q directly, because of initialization\r\n",
    "        self.Q = {}  \r\n",
    "        self.qval_init = .5\r\n",
    "        #\r\n",
    "        self.alpha = 0.05\r\n",
    "        self.gamma = 1.  # discount factor \r\n",
    "        pass\r\n",
    "\r\n",
    "    def get_random_action(self, state):\r\n",
    "        possible = [i for i, s in enumerate(state) if s == '-']\r\n",
    "        chosen = np.random.choice(possible)\r\n",
    "        return chosen \r\n",
    "\r\n",
    "    def get_action(self, state, random=False):\r\n",
    "        if random:\r\n",
    "            return self.get_random_action(state)\r\n",
    "        #\r\n",
    "        # Get the q-values for the state\r\n",
    "        q_vals = self.get_Qvalues(state)\r\n",
    "\r\n",
    "        # Get location of all max values and select a random one\r\n",
    "        max_q = np.round(max(q_vals), 7)\r\n",
    "        action_candidates = [i for i, qsa in enumerate(q_vals) if qsa == max_q]\r\n",
    "        print('action_cand: ', action_candidates, q_vals, max_q)\r\n",
    "        action = np.random.choice(action_candidates)\r\n",
    "        return action  # the  place to put 'O' or 'X\"\r\n",
    "\r\n",
    "    def get_Qvalues(self, state):\r\n",
    "        if state not in self.Q.keys():\r\n",
    "            values = np.array([self.qval_init if ox == '-' else float('-inf') for ox in state])\r\n",
    "            self.Q[state] = values  # register a new action-values\r\n",
    "        return self.Q[state]\r\n",
    "\r\n",
    "    def q_update(self, state, action, reward, new_state, done):\r\n",
    "        qsa = self.Q[state][action]  # Q(S,A)\r\n",
    "        pred = reward\r\n",
    "        if not done:\r\n",
    "            max_a = max(self.get_Qvalues(new_state))\r\n",
    "            pred += self.gamma * max_a\r\n",
    "        #\r\n",
    "        self.Q[state][action] += self.alpha * (pred - qsa)   # Q-learning or TD(0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "source": [
    "env = EnvTTT()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test running of the environment"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "source": [
    "def run(agents, update=False):\r\n",
    "    done = False\r\n",
    "    state = env.reset()\r\n",
    "    print('initial state: ', state)\r\n",
    "    while not done:\r\n",
    "        action = agents[0].get_random_action(state)\r\n",
    "        state, reward, done, info = env.step(action, agents[0].ox)\r\n",
    "        print(f'Turn: {agents[0].ox}, A: {action}, S: {state}, R: {reward}')\r\n",
    "        # env.render()\r\n",
    "\r\n",
    "        if done: \r\n",
    "            break \r\n",
    "\r\n",
    "        action = agents[1].get_random_action(state)\r\n",
    "        state, reward, done, info = env.step(action, agents[1].ox)\r\n",
    "        print(f'Turn: {agents[1].ox}, A: {action}, S: {state}, R: {reward}')\r\n",
    "        # env.render()\r\n",
    "        \r\n",
    "    print(f'Winner: {env.winner}')\r\n",
    "    env.render()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "source": [
    "# we need two agents to play the game\r\n",
    "agents = [QAgent('X'), QAgent('O')]\r\n",
    "\r\n",
    "run(agents)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "initial state:  ---------\n",
      "Turn: X, A: 5, S: -----X---, R: 0\n",
      "Turn: O, A: 4, S: ----OX---, R: 0\n",
      "Turn: X, A: 3, S: ---XOX---, R: 0\n",
      "Turn: O, A: 8, S: ---XOX--O, R: 0\n",
      "Turn: X, A: 0, S: X--XOX--O, R: 0\n",
      "Turn: O, A: 1, S: XO-XOX--O, R: 0\n",
      "Turn: X, A: 7, S: XO-XOX-XO, R: 0\n",
      "Turn: O, A: 2, S: XOOXOX-XO, R: 0\n",
      "Turn: X, A: 6, S: XOOXOXXXO, R: 1\n",
      "Winner: X\n",
      "XOO\n",
      "XOX\n",
      "XXO\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "source": [
    "# without learning, Q table is empty\r\n",
    "agents[0].Q"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "metadata": {},
     "execution_count": 120
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "source": [
    "def run_X(agents):\r\n",
    "    done = False\r\n",
    "    state = env.reset()\r\n",
    "    print('initial state: ', state)\r\n",
    "    while not done:\r\n",
    "        action = agents[0].get_action(state)\r\n",
    "        new_state, reward, done, info = env.step(action, agents[0].ox)\r\n",
    "        print(f'Turn: {agents[0].ox}, A: {action}, S: {state}, R: {reward}')\r\n",
    "        # env.render()\r\n",
    "\r\n",
    "        # agents[1].q_update(state, action, reward, new_state, done)\r\n",
    "\r\n",
    "        if done: \r\n",
    "            agents[0].q_update(state, action, reward, new_state, done)  # if game is over (win), then update\r\n",
    "            state = new_state\r\n",
    "            break \r\n",
    "\r\n",
    "        action = agents[1].get_random_action(state)\r\n",
    "        new_state, reward, done, info = env.step(action, agents[1].ox)\r\n",
    "        print(f'Turn: {agents[1].ox}, A: {action}, S: {state}, R: {reward}')\r\n",
    "        # env.render()\r\n",
    "\r\n",
    "        # Now, the opponant finished its move. Update.\r\n",
    "        agents[0].q_update(state, action, reward, new_state, done)\r\n",
    "\r\n",
    "        state = new_state \r\n",
    "\r\n",
    "    print(f'Winner: {env.winner}')\r\n",
    "    env.render()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "source": [
    "run_X(agents)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "initial state:  ---------\n",
      "action_cand:  [0, 1, 2, 3, 4, 5, 6, 7, 8] [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5] 0.5\n",
      "Turn: X, A: 0, S: ---------, R: 0\n",
      "Turn: O, A: 0, S: ---------, R: 0\n",
      "Winner: None\n",
      "X--\n",
      "---\n",
      "---\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "source": [
    "from collections import defaultdict\r\n",
    "import numpy as np \r\n",
    "\r\n",
    "q = defaultdict(lambda : np.ones((9,))*.5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "source": [
    "q['hello']"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5])"
      ]
     },
     "metadata": {},
     "execution_count": 125
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "source": [
    "q"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>,\n",
       "            {'hello': array([0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5])})"
      ]
     },
     "metadata": {},
     "execution_count": 126
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "source": [
    "a = np.array(['X', 'X', 'O'])\r\n",
    "\r\n",
    "b = a == 'X'\r\n",
    "b"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([ True,  True, False])"
      ]
     },
     "metadata": {},
     "execution_count": 131
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit"
  },
  "interpreter": {
   "hash": "5c2412a7f059a44a3ebbda245f298c96603c0f68fdadd5589b1fcd57ac7e1e28"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}